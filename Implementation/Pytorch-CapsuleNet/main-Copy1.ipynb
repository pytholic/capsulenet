{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from capsnet import CapsNet\n",
    "from data_loader import Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"/home/trojan/Desktop/dimentia/dataset/data_2categ/data_PGGAN/train\"\n",
    "valDir = \"/home/trojan/Desktop/dimentia/dataset/data_2categ/data_PGGAN/validation\"\n",
    "\n",
    "USE_CUDA = True if torch.cuda.is_available() else False\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 30\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dataset='dementia'):\n",
    "        if dataset == 'mnist':\n",
    "            # CNN (cnn)\n",
    "            self.cnn_in_channels = 1\n",
    "            self.cnn_out_channels = 256\n",
    "            self.cnn_kernel_size = 9\n",
    "\n",
    "            # Primary Capsule (pc)\n",
    "            self.pc_num_capsules = 8\n",
    "            self.pc_in_channels = 256\n",
    "            self.pc_out_channels = 32\n",
    "            self.pc_kernel_size = 9\n",
    "            self.pc_num_routes = 32 * 6 * 6\n",
    "\n",
    "            # Digit Capsule (dc)\n",
    "            self.dc_num_capsules = 10\n",
    "            self.dc_num_routes = 32 * 6 * 6\n",
    "            self.dc_in_channels = 8\n",
    "            self.dc_out_channels = 16\n",
    "\n",
    "            # Decoder\n",
    "            self.input_width = 28\n",
    "            self.input_height = 28\n",
    "\n",
    "        elif dataset == 'cifar10':\n",
    "            # CNN (cnn)\n",
    "            self.cnn_in_channels = 3\n",
    "            self.cnn_out_channels = 256\n",
    "            self.cnn_kernel_size = 9\n",
    "\n",
    "            # Primary Capsule (pc)\n",
    "            self.pc_num_capsules = 8\n",
    "            self.pc_in_channels = 256\n",
    "            self.pc_out_channels = 32\n",
    "            self.pc_kernel_size = 9\n",
    "            self.pc_num_routes = 32 * 8 * 8\n",
    "\n",
    "            # Digit Capsule (dc)\n",
    "            self.dc_num_capsules = 10\n",
    "            self.dc_num_routes = 32 * 8 * 8\n",
    "            self.dc_in_channels = 8\n",
    "            self.dc_out_channels = 16\n",
    "\n",
    "            # Decoder\n",
    "            self.input_width = 32\n",
    "            self.input_height = 32\n",
    "\n",
    "        elif dataset == 'dementia':\n",
    "            # CNN (cnn)\n",
    "            self.cnn_in_channels = 3\n",
    "            self.cnn_out_channels = 256\n",
    "            self.cnn_kernel_size = 9\n",
    "\n",
    "            # Primary Capsule (pc)\n",
    "            self.pc_num_capsules = 8\n",
    "            self.pc_in_channels = 256\n",
    "            self.pc_out_channels = 32\n",
    "            self.pc_kernel_size = 9\n",
    "            self.pc_num_routes = 32 * 6 * 6\n",
    "\n",
    "            # Digit Capsule (dc)\n",
    "            self.dc_num_capsules = 2\n",
    "            self.dc_num_routes = 32 * 6 * 6\n",
    "            self.dc_in_channels = 8\n",
    "            self.dc_out_channels = 16\n",
    "\n",
    "            # Decoder\n",
    "            self.input_width = 28\n",
    "            self.input_height = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch):\n",
    "    capsule_net = model\n",
    "    capsule_net.train()\n",
    "    n_batch = len(list(enumerate(train_loader)))\n",
    "    total_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        target = torch.sparse.torch.eye(2).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct = sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))\n",
    "        train_loss = loss.item()\n",
    "        total_loss += train_loss\n",
    "        if batch_id % 100 == 0:\n",
    "            tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
    "                epoch,\n",
    "                N_EPOCHS,\n",
    "                batch_id + 1,\n",
    "                n_batch,\n",
    "                correct / float(BATCH_SIZE),\n",
    "                train_loss / float(BATCH_SIZE)\n",
    "                ))\n",
    "    tqdm.write('Epoch: [{}/{}], train loss: {:.6f}'.format(epoch,N_EPOCHS,total_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def validation(capsule_net, val_loader, epoch):\n",
    "    capsule_net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    for batch_id, (data, target) in enumerate(val_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(2).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        correct += sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
    "                       np.argmax(target.data.cpu().numpy(), 1))\n",
    "\n",
    "    tqdm.write(\n",
    "        \"Epoch: [{}/{}], validation accuracy: {:.6f}, loss: {:.6f}\".format(epoch, N_EPOCHS, correct / len(val_loader.dataset),\n",
    "                                                                  val_loss / len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1218 [00:00<02:14,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [1/1218], train accuracy: 0.500000, loss: 0.112584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 103/1218 [00:06<01:14, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [101/1218], train accuracy: 0.625000, loss: 0.056708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 203/1218 [00:13<01:06, 15.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [201/1218], train accuracy: 0.750000, loss: 0.051568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 303/1218 [00:19<01:00, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [301/1218], train accuracy: 0.750000, loss: 0.050634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 403/1218 [00:26<00:54, 15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [401/1218], train accuracy: 0.875000, loss: 0.053285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 503/1218 [00:33<00:46, 15.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [501/1218], train accuracy: 0.500000, loss: 0.052383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 603/1218 [00:39<00:40, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [601/1218], train accuracy: 0.625000, loss: 0.049497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 703/1218 [00:46<00:34, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [701/1218], train accuracy: 0.875000, loss: 0.050362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 803/1218 [00:53<00:27, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [801/1218], train accuracy: 1.000000, loss: 0.050029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 903/1218 [00:59<00:20, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [901/1218], train accuracy: 0.750000, loss: 0.049622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1003/1218 [01:06<00:14, 15.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [1001/1218], train accuracy: 0.625000, loss: 0.049456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1103/1218 [01:12<00:07, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [1101/1218], train accuracy: 0.875000, loss: 0.048763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1203/1218 [01:19<00:00, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Batch: [1201/1218], train accuracy: 0.875000, loss: 0.050418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1218/1218 [01:20<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], train loss: 0.052306\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ed76a62c3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdementia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdementia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'test_loader'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(1)\n",
    "    dataset = 'dementia'\n",
    "    # dataset = 'mnist'\n",
    "    config = Config(dataset)\n",
    "    dementia = Dataset(dataset, BATCH_SIZE, trainDir, valDir)\n",
    "\n",
    "    capsule_net = CapsNet(config)\n",
    "    capsule_net = torch.nn.DataParallel(capsule_net)\n",
    "    if USE_CUDA:\n",
    "        capsule_net = capsule_net.cuda()\n",
    "    capsule_net = capsule_net.module\n",
    "\n",
    "    optimizer = torch.optim.Adam(capsule_net.parameters())\n",
    "\n",
    "    for e in range(1, N_EPOCHS + 1):\n",
    "        train(capsule_net, optimizer, dementia.train_loader, e)\n",
    "        validation(capsule_net, dementia.val_loader, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
